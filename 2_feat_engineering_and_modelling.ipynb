{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dadf2b61",
   "metadata": {
    "id": "dadf2b61"
   },
   "source": [
    "# Turbofan Engine Degradation - NASA Prognostics Center of Excellence \n",
    "\n",
    "This notebooks presents the development of an algorithm to predict the Remaining Useful Life of Turbofan engines. By the end, the obtained ML model has an XX accuracy\n",
    "\n",
    "It is divided into two main sections: Feature Engineering and Modelling.\n",
    "\n",
    "Through FE section I'll prepare data for training, create new features and delete unnecessary ones, and transform and normalize data.\n",
    "On modelling, I'll perform different tests with different models and datasets, choose the best version based on R2 score and optimize the hyperparameters with bayesian technique. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23e92ab7",
   "metadata": {
    "id": "23e92ab7"
   },
   "source": [
    "## Imports and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "227bb3c1",
   "metadata": {
    "id": "227bb3c1"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29b47a41",
   "metadata": {
    "id": "29b47a41"
   },
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "# pd.set_option('display.max_rows', None)\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86a4875a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 270
    },
    "id": "86a4875a",
    "outputId": "273cef20-cbc8-4c51-9fb4-93f06845bf8f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>unit_number</th>\n",
       "      <th>time</th>\n",
       "      <th>os_1</th>\n",
       "      <th>os_2</th>\n",
       "      <th>os_3</th>\n",
       "      <th>sm_1</th>\n",
       "      <th>sm_2</th>\n",
       "      <th>sm_3</th>\n",
       "      <th>sm_4</th>\n",
       "      <th>sm_5</th>\n",
       "      <th>sm_6</th>\n",
       "      <th>sm_7</th>\n",
       "      <th>sm_8</th>\n",
       "      <th>sm_9</th>\n",
       "      <th>sm_10</th>\n",
       "      <th>sm_11</th>\n",
       "      <th>sm_12</th>\n",
       "      <th>sm_13</th>\n",
       "      <th>sm_14</th>\n",
       "      <th>sm_15</th>\n",
       "      <th>sm_16</th>\n",
       "      <th>sm_17</th>\n",
       "      <th>sm_18</th>\n",
       "      <th>sm_19</th>\n",
       "      <th>sm_20</th>\n",
       "      <th>sm_21</th>\n",
       "      <th>RUL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>-0.0007</td>\n",
       "      <td>-0.0004</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>641.82</td>\n",
       "      <td>1589.70</td>\n",
       "      <td>1400.60</td>\n",
       "      <td>14.62</td>\n",
       "      <td>21.61</td>\n",
       "      <td>554.36</td>\n",
       "      <td>2388.06</td>\n",
       "      <td>9046.19</td>\n",
       "      <td>1.3</td>\n",
       "      <td>47.47</td>\n",
       "      <td>521.66</td>\n",
       "      <td>2388.02</td>\n",
       "      <td>8138.62</td>\n",
       "      <td>8.4195</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.06</td>\n",
       "      <td>23.4190</td>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0019</td>\n",
       "      <td>-0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.15</td>\n",
       "      <td>1591.82</td>\n",
       "      <td>1403.14</td>\n",
       "      <td>14.62</td>\n",
       "      <td>21.61</td>\n",
       "      <td>553.75</td>\n",
       "      <td>2388.04</td>\n",
       "      <td>9044.07</td>\n",
       "      <td>1.3</td>\n",
       "      <td>47.49</td>\n",
       "      <td>522.28</td>\n",
       "      <td>2388.07</td>\n",
       "      <td>8131.49</td>\n",
       "      <td>8.4318</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>39.00</td>\n",
       "      <td>23.4236</td>\n",
       "      <td>190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.0043</td>\n",
       "      <td>0.0003</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.35</td>\n",
       "      <td>1587.99</td>\n",
       "      <td>1404.20</td>\n",
       "      <td>14.62</td>\n",
       "      <td>21.61</td>\n",
       "      <td>554.26</td>\n",
       "      <td>2388.08</td>\n",
       "      <td>9052.94</td>\n",
       "      <td>1.3</td>\n",
       "      <td>47.27</td>\n",
       "      <td>522.42</td>\n",
       "      <td>2388.03</td>\n",
       "      <td>8133.23</td>\n",
       "      <td>8.4178</td>\n",
       "      <td>0.03</td>\n",
       "      <td>390</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.95</td>\n",
       "      <td>23.3442</td>\n",
       "      <td>189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0007</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.35</td>\n",
       "      <td>1582.79</td>\n",
       "      <td>1401.87</td>\n",
       "      <td>14.62</td>\n",
       "      <td>21.61</td>\n",
       "      <td>554.45</td>\n",
       "      <td>2388.11</td>\n",
       "      <td>9049.48</td>\n",
       "      <td>1.3</td>\n",
       "      <td>47.13</td>\n",
       "      <td>522.86</td>\n",
       "      <td>2388.08</td>\n",
       "      <td>8133.83</td>\n",
       "      <td>8.3682</td>\n",
       "      <td>0.03</td>\n",
       "      <td>392</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.88</td>\n",
       "      <td>23.3739</td>\n",
       "      <td>188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>-0.0019</td>\n",
       "      <td>-0.0002</td>\n",
       "      <td>100.0</td>\n",
       "      <td>518.67</td>\n",
       "      <td>642.37</td>\n",
       "      <td>1582.85</td>\n",
       "      <td>1406.22</td>\n",
       "      <td>14.62</td>\n",
       "      <td>21.61</td>\n",
       "      <td>554.00</td>\n",
       "      <td>2388.06</td>\n",
       "      <td>9055.15</td>\n",
       "      <td>1.3</td>\n",
       "      <td>47.28</td>\n",
       "      <td>522.19</td>\n",
       "      <td>2388.04</td>\n",
       "      <td>8133.80</td>\n",
       "      <td>8.4294</td>\n",
       "      <td>0.03</td>\n",
       "      <td>393</td>\n",
       "      <td>2388</td>\n",
       "      <td>100.0</td>\n",
       "      <td>38.90</td>\n",
       "      <td>23.4044</td>\n",
       "      <td>187</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   unit_number  time    os_1    os_2   os_3    sm_1    sm_2     sm_3     sm_4  \\\n",
       "0            1     1 -0.0007 -0.0004  100.0  518.67  641.82  1589.70  1400.60   \n",
       "1            1     2  0.0019 -0.0003  100.0  518.67  642.15  1591.82  1403.14   \n",
       "2            1     3 -0.0043  0.0003  100.0  518.67  642.35  1587.99  1404.20   \n",
       "3            1     4  0.0007  0.0000  100.0  518.67  642.35  1582.79  1401.87   \n",
       "4            1     5 -0.0019 -0.0002  100.0  518.67  642.37  1582.85  1406.22   \n",
       "\n",
       "    sm_5   sm_6    sm_7     sm_8     sm_9  sm_10  sm_11   sm_12    sm_13  \\\n",
       "0  14.62  21.61  554.36  2388.06  9046.19    1.3  47.47  521.66  2388.02   \n",
       "1  14.62  21.61  553.75  2388.04  9044.07    1.3  47.49  522.28  2388.07   \n",
       "2  14.62  21.61  554.26  2388.08  9052.94    1.3  47.27  522.42  2388.03   \n",
       "3  14.62  21.61  554.45  2388.11  9049.48    1.3  47.13  522.86  2388.08   \n",
       "4  14.62  21.61  554.00  2388.06  9055.15    1.3  47.28  522.19  2388.04   \n",
       "\n",
       "     sm_14   sm_15  sm_16  sm_17  sm_18  sm_19  sm_20    sm_21  RUL  \n",
       "0  8138.62  8.4195   0.03    392   2388  100.0  39.06  23.4190  191  \n",
       "1  8131.49  8.4318   0.03    392   2388  100.0  39.00  23.4236  190  \n",
       "2  8133.23  8.4178   0.03    390   2388  100.0  38.95  23.3442  189  \n",
       "3  8133.83  8.3682   0.03    392   2388  100.0  38.88  23.3739  188  \n",
       "4  8133.80  8.4294   0.03    393   2388  100.0  38.90  23.4044  187  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_raw = pd.read_csv('./data/df_train_cleaned.csv', index_col=0)\n",
    "df_test_raw = pd.read_csv('./data/df_test_cleaned.csv', index_col=0)\n",
    "\n",
    "df_train_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79439a3b",
   "metadata": {
    "id": "79439a3b"
   },
   "source": [
    "---\n",
    "## Feature Engineering\n",
    "Along this section, data will be transformed for obtaining better accuracy on the model.\n",
    "The problems we'll try to solve and the approach are the following ones:\n",
    "\n",
    "1. Not equally distributed data --> Perform normalization;\n",
    "    \n",
    "2. Columns with constant values --> Remove this column as they don't add information;\n",
    "    \n",
    "3. Past measurements helps on predicting RULs? --> Add lagged data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "430fcf69",
   "metadata": {
    "id": "430fcf69"
   },
   "outputs": [],
   "source": [
    "def remove_cols(df, cols):\n",
    "    \"\"\"\n",
    "    Drop selected cols.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): the input DataFrame containing the time series data.\n",
    "    - cols (list): list of cols to drop.\n",
    "\n",
    "    Returns\n",
    "        A new DataFrame with the cols dropped.\n",
    "    \"\"\"\n",
    "    df.drop(columns=cols, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a088521",
   "metadata": {
    "id": "8a088521"
   },
   "outputs": [],
   "source": [
    "def normalize_data(df, scaler):\n",
    "    \"\"\"\n",
    "    Normalize dataframe.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): the input DataFrame containing the time series data.\n",
    "    - scaler: model to normalize the data.\n",
    "\n",
    "    Returns\n",
    "        A new DataFrame with the normalized features.\n",
    "    \"\"\"\n",
    "    normalized_data = scaler.fit_transform(df.drop(columns='RUL'))\n",
    "    df_norm = pd.DataFrame(normalized_data, columns=df.drop(columns='RUL').columns)\n",
    "    df_norm['RUL'] = df['RUL']\n",
    "    return df_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f539c4dd",
   "metadata": {
    "id": "f539c4dd"
   },
   "outputs": [],
   "source": [
    "def add_lagged_measures(df, qtd):\n",
    "    \"\"\"\n",
    "    Create lagged features with the mean value of 'qtd' past rows.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): the input DataFrame containing the time series data.\n",
    "    - qtd (int): the number of past rows to include in the lagged features.\n",
    "\n",
    "    Returns\n",
    "        A new DataFrame with the lagged features added.\n",
    "    \"\"\"\n",
    "    df_lagged = df.copy()\n",
    "    cols_sm = filter_col = [col for col in df_lagged if col.startswith('sm_')]\n",
    "\n",
    "    for sensor in cols_sm:\n",
    "      for i in range(1, qtd+1):\n",
    "          df_lagged[f'L_{sensor}_{i}'] = df_lagged[f'{sensor}'].shift(i).rolling(window=i).mean()\n",
    "\n",
    "    # Drop the rows with missing values\n",
    "    df_lagged.dropna(inplace=True)\n",
    "    return df_lagged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eba2489",
   "metadata": {
    "id": "0eba2489"
   },
   "source": [
    "# 2. Modelling\n",
    "1. Define metrics and evaluation function\n",
    "2. Models\n",
    "3. Cross validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "66e6626b",
   "metadata": {
    "id": "66e6626b"
   },
   "outputs": [],
   "source": [
    "def prepare_train_test_dfs(df_train, df_test):\n",
    "    \"\"\"\n",
    "    Split and organize the data for trainning.\n",
    "\n",
    "    Parameters:\n",
    "    - df_train (pd.DataFrame)\n",
    "    - df_test (pd.DataFrame)\n",
    "\n",
    "    Returns\n",
    "        The data splitted.\n",
    "    \"\"\"\n",
    "    X_train = df_train.drop(columns='RUL')\n",
    "    y_train = df_train['RUL']\n",
    "\n",
    "    X_test = df_test.drop(columns='RUL')\n",
    "    y_test = df_test['RUL']\n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "10ebffad",
   "metadata": {
    "id": "10ebffad"
   },
   "outputs": [],
   "source": [
    "def test_models(df_train, df_test, models):\n",
    "    \"\"\"\n",
    "    Iterates over a list of models and select the one with highest accuracy..\n",
    "\n",
    "    Parameters:\n",
    "    - df_train (pd.DataFrame)\n",
    "    - df_test (pd.DataFrame)\n",
    "    - model (tuple): selected models\n",
    "\n",
    "    Returns\n",
    "        Results of perfomance of each model and the best one.\n",
    "    \"\"\"\n",
    "    X_train, y_train, X_test, y_test = prepare_train_test_dfs(df_train, df_test)\n",
    "    \n",
    "    model_results = []\n",
    "    best_rmse = 100\n",
    "    for name, model in models:\n",
    "        regressor = model.fit(X_train, y_train)\n",
    "        predicted = regressor.predict(X_test)\n",
    "        cv_score = cross_val_score(regressor, X_train, y_train, cv=5, scoring='r2')\n",
    "        cv2_rmse = np.sqrt(cv_score)\n",
    "    \n",
    "        if cv2_rmse.mean() < best_rmse:\n",
    "            best_model = model\n",
    "        \n",
    "        model_results.append([name, cv_score.mean(), cv2_rmse.mean()])\n",
    "\n",
    "    df_model_results = pd.DataFrame(model_results, columns=['model', 'cv_score_r2', 'cv_score_rmse'])\n",
    "    \n",
    "    return df_model_results, best_model   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c6fd3ef",
   "metadata": {
    "id": "2c6fd3ef"
   },
   "outputs": [],
   "source": [
    "models = [\n",
    "      ('LinReg', LinearRegression()), \n",
    "      ('RF', RandomForestRegressor()),\n",
    "      ('SVR', SVR()), \n",
    "      ('XGB', XGBRegressor())\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56c64b72",
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "56c64b72",
    "outputId": "aae1cc47-f885-485d-c938-cfffd80c476f",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[169.2392208  174.89789473 161.41324018 ...  51.1843808   48.20099162\n",
      "  32.41571624]\n",
      "[192.15 200.44 187.29 ...  24.13  22.39  16.81]\n",
      "[104.20218973 104.15845018 104.14510509 ... 101.11804733 101.07283447\n",
      " 101.03827535]\n",
      "[189.33284   208.07828   188.0731    ...  27.2548     17.822811\n",
      "  10.3357935]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(    model  cv_score_r2  cv_score_rmse\n",
       " 0  LinReg     0.645533       0.802104\n",
       " 1      RF     0.568721       0.749565\n",
       " 2     SVR    -0.009119            NaN\n",
       " 3     XGB     0.609482       0.777592,\n",
       " XGBRegressor(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, gpu_id=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=None, max_bin=None,\n",
       "              max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=None, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              n_estimators=100, n_jobs=None, num_parallel_tree=None,\n",
       "              predictor=None, random_state=None, ...))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_results, model = test_models(df_train_raw, df_test_raw, models)\n",
    "df_results, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc82d16",
   "metadata": {
    "id": "4dc82d16"
   },
   "source": [
    "---\n",
    "## Testing combinations of data transformation\n",
    "1. Without unnecessary columns\n",
    "2. Without unnecessary columns and normalize others\n",
    "3. Without unnecessary columns, normalize others and added lagged values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e11cd3a6",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "e11cd3a6"
   },
   "outputs": [],
   "source": [
    "def select_df(df_train, df_test, model):\n",
    "    \"\"\"\n",
    "    Pipeline of trainning and evaluateing the model and dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - df_train (pd.DataFrame)\n",
    "    - df_test (pd.DataFrame)\n",
    "    - model : selected model\n",
    "\n",
    "    Returns\n",
    "        Results of perfomance of the model.\n",
    "    \"\"\"\n",
    "    X_train, y_train, X_test, y_test = prepare_train_test_dfs(df_train, df_test)\n",
    "    regressor = model.fit(X_train, y_train)\n",
    "    predicted = regressor.predict(X_test)\n",
    "\n",
    "    cv_score = cross_val_score(regressor, X_train, y_train, cv=5, scoring='r2')\n",
    "    mse = mean_squared_error(y_test, predicted)\n",
    "    rmse = np.sqrt(mse)\n",
    "    cv2_rmse = np.sqrt(cv_score).mean()\n",
    "    \n",
    "    results = [cv2_rmse, cv_score]\n",
    "    return results "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f84689f",
   "metadata": {
    "id": "6f84689f"
   },
   "source": [
    "### 1. Remove unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c18a1278",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "c18a1278"
   },
   "outputs": [],
   "source": [
    "df_train = df_train_raw.copy()\n",
    "df_test = df_test_raw.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27e2dd87",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "27e2dd87"
   },
   "outputs": [],
   "source": [
    "unnecessary_cols = ['unit_number', 'os_3', 'sm_1', 'sm_5', 'sm_10', 'sm_16', 'sm_18', 'sm_19']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5be1b8c3",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "5be1b8c3"
   },
   "outputs": [],
   "source": [
    "train_no_cols = df_train.copy().pipe(remove_cols, unnecessary_cols)\n",
    "test_no_cols = df_test.copy().pipe(remove_cols, unnecessary_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e0fbf38",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0e0fbf38",
    "outputId": "060f7db0-7c80-48ac-9948-5154576f0ca3",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8119281486953694\n"
     ]
    }
   ],
   "source": [
    "result_norm_no_cols = select_df(train_no_cols, test_no_cols, model)\n",
    "print(result_norm_no_cols[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24f0ad0",
   "metadata": {
    "id": "b24f0ad0"
   },
   "source": [
    "### 2. Remove unnecessary columns and perform normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "69adc837",
   "metadata": {
    "id": "69adc837",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "train_norm_no_cols = (df_train.copy().pipe(remove_cols, unnecessary_cols)\n",
    "                               .pipe(normalize_data, StandardScaler()))\n",
    "test_norm_no_cols = (df_test.copy().pipe(remove_cols, unnecessary_cols)\n",
    "                             .pipe(normalize_data, StandardScaler()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48e3cc9b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "48e3cc9b",
    "outputId": "61044c28-6127-4130-b5ad-920bdc982073"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8117736895146741\n"
     ]
    }
   ],
   "source": [
    "result_norm_no_cols = select_df(train_norm_no_cols, test_norm_no_cols, model)\n",
    "print(result_norm_no_cols[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1e9bfc",
   "metadata": {
    "id": "cd1e9bfc"
   },
   "source": [
    "### 3. Remove unnecessary columns, perform normalization and add lagged data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d10c0f7",
   "metadata": {
    "id": "7d10c0f7"
   },
   "outputs": [],
   "source": [
    "NUM_LAGGED_ROWS = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b4b66a63",
   "metadata": {
    "id": "b4b66a63"
   },
   "outputs": [],
   "source": [
    "train_lag_norm_no_cols = (df_train.copy().pipe(remove_cols, unnecessary_cols)\n",
    "                               .pipe(normalize_data, StandardScaler())\n",
    "                               .pipe(add_lagged_measures, NUM_LAGGED_ROWS))\n",
    "test_lag_norm_no_cols = (df_test.copy().pipe(remove_cols, unnecessary_cols)\n",
    "                             .pipe(normalize_data, StandardScaler())\n",
    "                             .pipe(add_lagged_measures, NUM_LAGGED_ROWS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "523bae64",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "523bae64",
    "outputId": "717bd8bf-3d5f-49b3-99db-a95d5cdfc324"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8114351240939328\n"
     ]
    }
   ],
   "source": [
    "result_norm_no_cols = select_df(train_lag_norm_no_cols, test_lag_norm_no_cols, model)\n",
    "print(result_norm_no_cols[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8ab60e",
   "metadata": {
    "id": "2a8ab60e"
   },
   "source": [
    "#### Find the best amount of lagged measurements to model perfomance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bdf3a201",
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "bdf3a201",
    "outputId": "6d12909e-c233-4ea2-ffcd-d46250b464d7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0.0\n",
      "2\n",
      "-0.0032371469398746466\n",
      "3\n",
      "-0.007443144578898386\n",
      "4\n",
      "-0.000628285427333175\n",
      "5\n",
      "0.0021745834404879316\n",
      "6\n",
      "-0.004326854538539204\n",
      "7\n",
      "-0.001521085224907348\n",
      "8\n",
      "-0.0010531438000520588\n",
      "9\n",
      "-0.001321087414721811\n",
      "10\n",
      "-0.0011944088387355611\n",
      "11\n",
      "0.006733656614853922\n",
      "12\n",
      "-0.0012284470073228304\n",
      "13\n",
      "0.003684382793294194\n",
      "14\n",
      "0.003729776385174799\n",
      "15\n",
      "0.003116734299477786\n",
      "16\n",
      "-0.0039493481751234905\n",
      "17\n",
      "0.0029570198805801695\n",
      "18\n",
      "0.003118144703433523\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [23]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m df_train_lagged \u001b[38;5;241m=\u001b[39m df_train_lag_opt\u001b[38;5;241m.\u001b[39mpipe(add_lagged_measures, NUM_LAGGED_ROWS)\n\u001b[0;32m      9\u001b[0m df_test_lagged \u001b[38;5;241m=\u001b[39m df_test_lag_opt\u001b[38;5;241m.\u001b[39mpipe(add_lagged_measures, NUM_LAGGED_ROWS)\n\u001b[1;32m---> 11\u001b[0m rmse_atual \u001b[38;5;241m=\u001b[39m \u001b[43mselect_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_train_lagged\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_test_lagged\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m delta \u001b[38;5;241m=\u001b[39m rmse_atual[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m past_rmse\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(NUM_LAGGED_ROWS)\n",
      "Input \u001b[1;32mIn [13]\u001b[0m, in \u001b[0;36mselect_df\u001b[1;34m(df_train, df_test, model)\u001b[0m\n\u001b[0;32m     14\u001b[0m regressor \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m     15\u001b[0m predicted \u001b[38;5;241m=\u001b[39m regressor\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[1;32m---> 17\u001b[0m cv_score \u001b[38;5;241m=\u001b[39m \u001b[43mcross_val_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mregressor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m mse \u001b[38;5;241m=\u001b[39m mean_squared_error(y_test, predicted)\n\u001b[0;32m     19\u001b[0m rmse \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msqrt(mse)\n",
      "File \u001b[1;32m~\\conda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:509\u001b[0m, in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    506\u001b[0m \u001b[38;5;66;03m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[0;32m    507\u001b[0m scorer \u001b[38;5;241m=\u001b[39m check_scoring(estimator, scoring\u001b[38;5;241m=\u001b[39mscoring)\n\u001b[1;32m--> 509\u001b[0m cv_results \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    510\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    511\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    512\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    513\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    514\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    515\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    516\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    517\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    518\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    519\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    520\u001b[0m \u001b[43m    \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    521\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    522\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cv_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\conda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:267\u001b[0m, in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[0;32m    265\u001b[0m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[0;32m    266\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[1;32m--> 267\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    269\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    270\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    271\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    272\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscorers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    273\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    274\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    275\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    276\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    277\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    278\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    279\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_times\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    280\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_estimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_estimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    281\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    282\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    283\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    284\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    286\u001b[0m _warn_about_fit_failures(results, error_score)\n\u001b[0;32m    288\u001b[0m \u001b[38;5;66;03m# For callabe scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[0;32m    290\u001b[0m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n",
      "File \u001b[1;32m~\\conda\\lib\\site-packages\\joblib\\parallel.py:1046\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1043\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1044\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_original_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1046\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdispatch_one_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m   1047\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m   1049\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pre_dispatch \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   1050\u001b[0m     \u001b[38;5;66;03m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   1051\u001b[0m     \u001b[38;5;66;03m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   1052\u001b[0m     \u001b[38;5;66;03m# consumption.\u001b[39;00m\n",
      "File \u001b[1;32m~\\conda\\lib\\site-packages\\joblib\\parallel.py:861\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    859\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    860\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 861\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    862\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32m~\\conda\\lib\\site-packages\\joblib\\parallel.py:779\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    777\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    778\u001b[0m     job_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs)\n\u001b[1;32m--> 779\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    780\u001b[0m     \u001b[38;5;66;03m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    781\u001b[0m     \u001b[38;5;66;03m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    782\u001b[0m     \u001b[38;5;66;03m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    783\u001b[0m     \u001b[38;5;66;03m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    784\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jobs\u001b[38;5;241m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32m~\\conda\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_async\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, callback\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;124;03m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mImmediateResult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32m~\\conda\\lib\\site-packages\\joblib\\_parallel_backends.py:572\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    569\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch):\n\u001b[0;32m    570\u001b[0m     \u001b[38;5;66;03m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    571\u001b[0m     \u001b[38;5;66;03m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 572\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\conda\\lib\\site-packages\\joblib\\parallel.py:262\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 262\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    263\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32m~\\conda\\lib\\site-packages\\joblib\\parallel.py:262\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    259\u001b[0m     \u001b[38;5;66;03m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    260\u001b[0m     \u001b[38;5;66;03m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    261\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m parallel_backend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 262\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    263\u001b[0m                 \u001b[38;5;28;01mfor\u001b[39;00m func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems]\n",
      "File \u001b[1;32m~\\conda\\lib\\site-packages\\sklearn\\utils\\fixes.py:216\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig):\n\u001b[1;32m--> 216\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\conda\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:680\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    678\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    679\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 680\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, y_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    682\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    683\u001b[0m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[0;32m    684\u001b[0m     fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[1;32m~\\conda\\lib\\site-packages\\xgboost\\core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    618\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    619\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 620\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\conda\\lib\\site-packages\\xgboost\\sklearn.py:1025\u001b[0m, in \u001b[0;36mXGBModel.fit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[0;32m   1014\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1016\u001b[0m (\n\u001b[0;32m   1017\u001b[0m     model,\n\u001b[0;32m   1018\u001b[0m     metric,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1023\u001b[0m     xgb_model, eval_metric, params, early_stopping_rounds, callbacks\n\u001b[0;32m   1024\u001b[0m )\n\u001b[1;32m-> 1025\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1026\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1027\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dmatrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1028\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_num_boosting_rounds\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1029\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1030\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1031\u001b[0m \u001b[43m    \u001b[49m\u001b[43mevals_result\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mevals_result\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1032\u001b[0m \u001b[43m    \u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1033\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetric\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1034\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1035\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxgb_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1036\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1037\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1039\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_evaluation_result(evals_result)\n\u001b[0;32m   1040\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\conda\\lib\\site-packages\\xgboost\\core.py:620\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    618\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    619\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 620\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\conda\\lib\\site-packages\\xgboost\\training.py:185\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m \u001b[43mbst\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    187\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\conda\\lib\\site-packages\\xgboost\\core.py:1918\u001b[0m, in \u001b[0;36mBooster.update\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m   1915\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_dmatrix_features(dtrain)\n\u001b[0;32m   1917\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1918\u001b[0m     _check_call(\u001b[43m_LIB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mXGBoosterUpdateOneIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1919\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mctypes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mc_int\u001b[49m\u001b[43m(\u001b[49m\u001b[43miteration\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1920\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mdtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhandle\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   1921\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1922\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(dtrain, output_margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df_lag_optmization = pd.DataFrame(columns=['days', 'rmse', 'delta'])\n",
    "df_train_lag_opt = train_norm_no_cols.copy()\n",
    "df_test_lag_opt = test_norm_no_cols.copy()\n",
    "\n",
    "past_rmse = result_norm_no_cols[0]\n",
    "for i in range(1,25):\n",
    "    NUM_LAGGED_ROWS = i\n",
    "    df_train_lagged = df_train_lag_opt.pipe(add_lagged_measures, NUM_LAGGED_ROWS)\n",
    "    df_test_lagged = df_test_lag_opt.pipe(add_lagged_measures, NUM_LAGGED_ROWS)\n",
    "    \n",
    "    rmse_atual = select_df(df_train_lagged, df_test_lagged, model)\n",
    "    delta = rmse_atual[0] - past_rmse\n",
    "    print(NUM_LAGGED_ROWS)\n",
    "    print(delta)\n",
    "    df_actual_lag = pd.DataFrame({'days': NUM_LAGGED_ROWS, 'rmse': rmse_atual[0], 'delta': delta}, index=[0])\n",
    "    past_rmse = rmse_atual[0]\n",
    "    df_lag_optmization = pd.concat([df_lag_optmization, df_actual_lag])\n",
    "\n",
    "df_lag_optmization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xcamlIzPGn9w",
   "metadata": {
    "id": "xcamlIzPGn9w",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "fig, ax = plt.subplots(figsize=(12,4))\n",
    "sns.lineplot(y=f'delta', x='days', palette='hls', data=df_lag_optmization, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079cf0f8",
   "metadata": {
    "id": "079cf0f8"
   },
   "source": [
    "## Hyperparameters optimization \n",
    "After decideing which model and dataset, we'll improve model accuracy by utilizing Bayesian Optmization. \n",
    "This technique Bayesian optimization works by constructing a posterior distribution of functions (gaussian process) that best describes the function you want to optimize. As the number of observations grows, the posterior distribution improves, and the algorithm becomes more certain of which regions in parameter space are worth exploring and which are not.\n",
    "As you iterate over and over, the algorithm balances its needs of exploration and exploitation taking into account what it knows about the target function. At each step a Gaussian Process is fitted to the known samples (points previously explored), and the posterior distribution, combined with a exploration strategy (such as UCB (Upper Confidence Bound), or EI (Expected Improvement)), are used to determine the next point that should be explored.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968a912a",
   "metadata": {
    "id": "968a912a"
   },
   "outputs": [],
   "source": [
    "def train_model(max_depth, \n",
    "                ntrees,\n",
    "                min_rows, \n",
    "                learn_rate, \n",
    "                sample_rate, \n",
    "                col_sample_rate):\n",
    "    params = {\n",
    "        'max_depth': int(max_depth),\n",
    "        'ntrees': int(ntrees),\n",
    "        'min_rows': int(min_rows),\n",
    "        'learn_rate':learn_rate,\n",
    "        'sample_rate':sample_rate,\n",
    "        'col_sample_rate':col_sample_rate\n",
    "    }\n",
    "    model = LinearRegression() ###### \n",
    "    model.train(x=X_train, y=_train, training_frame=train)\n",
    "    return -model.rmse()\n",
    "\n",
    "bounds = {\n",
    "    'max_depth':(5,10),\n",
    "    'ntrees': (100,500),\n",
    "    'min_rows':(10,30),\n",
    "    'learn_rate':(0.001, 0.01),\n",
    "    'sample_rate':(0.5,0.8),\n",
    "    'col_sample_rate':(0.5,0.8)\n",
    "}\n",
    "optimizer = BayesianOptimization(\n",
    "    f=train_model,\n",
    "    pbounds=bounds,\n",
    "    random_state=1,\n",
    ")\n",
    "optimizer.maximize(init_points=10, n_iter=50)\n",
    "optimizer.max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6a9fac",
   "metadata": {
    "id": "4e6a9fac"
   },
   "outputs": [],
   "source": [
    "# https://medium.com/@beniciowg/como-tunar-hiperpar%C3%A2metros-com-otimiza%C3%A7%C3%A3o-bayesiana-5687fd51370f\n",
    "\n",
    "# importando o BayesSearchCV\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Categorical, Integer\n",
    "# instanciando o algoritmo de classificao XGBoost\n",
    "clf = RandomForestRegressor()\n",
    "# definindo as faixas de valores a serem testadas para cada hiperparmetro a ser otimizado\n",
    "space = {\n",
    "    'max_depth':(5,10)\n",
    "}\n",
    "# instanciando o algoritmo de otimizao e definindo alguns parmetros dele\n",
    "opt = BayesSearchCV(clf, space, n_iter=5, random_state=42, cv=5,\n",
    "                    return_train_score=True, scoring='r2', refit=True)\n",
    "# executando o modelo de otimizao\n",
    "opt.fit(X_train, y_train)\n",
    "# aplicando o modelo nos dados de teste\n",
    "y_pred_opt = opt.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred_opt)\n",
    "# imprimindo os resultados de avaliao\n",
    "print(mse)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
